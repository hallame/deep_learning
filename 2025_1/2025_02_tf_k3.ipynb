{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Глубокое обучение 2025\n",
    "\n",
    "\n",
    "## Занятие 2. Глубокое обучение с Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание виртуального окружения для Jupyter Notebook\n",
    "\n",
    "Последовательность действий:\n",
    "* `conda create -n <имя> python=3.11.9` или через Anaconda Navigator\n",
    "* `conda activate <имя>` или через Anaconda Navigator\n",
    "* `conda install <пакеты>` или через Anaconda Navigator или `pip install <пакеты>`\n",
    "* `conda install ipykernel` или через Anaconda Navigator\n",
    "* `python -m ipykernel install --user --name=<имя>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем предупреждения и импортируем библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убираем предупреждающие сообщения\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "from keras import datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Пример\n",
    "Будем использовать набор данных `Fashion-MNIST`, который состоит из изображений размером 28x28 пикселей для 10 классов модных товаров.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fmnist_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", \n",
    "                  3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n",
    "                  6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
    "\n",
    "# Выберем случайные изображения\n",
    "from random import randint\n",
    "fig, axes = plt.subplots(1, 5,  figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    n = randint(0,60000) # 70000\n",
    "    axes[i].imshow(X_train[n], cmap=plt.cm.gray_r) # .reshape(28, 28)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].set_xlabel(\"{}\".format(fmnist_classes[y_train[n]]))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Препроцессинг\n",
    "#### Изменение формы (reshaping)\n",
    "Чтобы иметь возможность передавать эти данные через нейронную сеть, форма входных данных должна соответствовать форме входного слоя.\n",
    "Для обычных плотных слоев это будет плоский массив. Можно использовать:\n",
    "\n",
    "* [numpy.reshape()](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)\n",
    "* [слои Keras](https://keras.io/api/layers/reshaping_layers/), например, [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изменение масштаба (rescaling)\n",
    "Изменение масштаба данных помогает при обучении нейронной сети и приведет к более быстрой сходимости.\n",
    "Вы можете использовать минимальное и максимальное масштабирование до [0,1] или стандартизацию (среднее значение 0, стандартное отклонение 1).\n",
    "Используем здесь простое деление на максимально возможное значение. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test  = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Форматирование меток\n",
    "Для многоклассовой классификации наш выходной слой обычно будет иметь один выходной нейрон для каждого класса. Поэтому нам необходимо выполнить прямое кодирование меток (one-hot-encoding). Например, классу '4' соответствует вектор `[0,0,0,0,1,0,...]` с единицей на пятой позиции (метки кодируются с нуля).\n",
    "\n",
    "Для этого в Keras есть вспомогательная функция [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1, 28*28))\n",
    "X_test  = X_test.reshape((-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение на обучающую, тестовую и проверочную выборки\n",
    "Если наборы данных достаточно велики, то обычно используются простые алгоритмы разделения. Для небольших наборов данных также можно использовать перекрестную валидацию, но можно столкнуться с высокой дисперсией результатов.\n",
    "\n",
    "Разделим обучающую выборку на набор для обучения и проверки (валидации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xf_train, X_val, yf_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, train_size=50000, \n",
    "                     shuffle=True, stratify=y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf_train.shape, yf_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Построение последовательных моделей нейронных сетей\n",
    "* [Последовательные](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) модели `Tensorflow` — это самый простой вид нейронных сетей. Они состоят из ряда слоев, идущих один за другим.\n",
    "* В `Tensorflow` определено [много типов слоев](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "* Пока будем использовать только плотные [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) (полносвязные) слои. В плотных слоях имеется несколько важных настроек:\n",
    "    * __units__: количество узлов (нейронов)\n",
    "    * __activation__: [функция активации](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    "    * __kernel_initializer__: как [инициализировать веса](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)\n",
    "    * __kernel_regularizer__: применять ли L1/L2 [регуляризацию](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)\n",
    "    \n",
    "``` python\n",
    "keras.layers.Dense(\n",
    "    units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "    activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующую простую сеть с одним скрытым слоем:\n",
    "* метод `Sequential.add()` добавляет слой в сеть\n",
    "* также можно передать в конструктор массив слоев: `Sequential([layers])`\n",
    "* используется активация `ReLU` для скрытого слоя и `SoftMax` для выходного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Входной слой\n",
    "Обратите внимание, что входной слой может быть определен с помощью параметра `input_shape`. В качестве альтернативы также можно добавить явный входной слой `InputLayer` с параметром `shape`.\n",
    "В нашем случае данные представляют собой плоский массив из 28*28 входов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Слои активации\n",
    "Большинство функций активации, инициализаторов и регуляризаторов можно указать в виде  ключевых слов. Если нужен больший контроль над нейронной сетью, то можно указать активацию как отдельный слой. Тогда плотный слой будет использовать линейную активацию, а в следующем слое будет применяться выбранная активация.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.ReLU(negative_slope=0.1)) # слой leaky ReLU\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Краткое описание модели\n",
    "- вызов `model.summary()` выводит краткое описание модели по слоям\n",
    "    - скрытый слой 1: (28 * 28 + 1) * 512 = 401920\n",
    "    - скрытый слой 2: (512 + 1) * 512 = 262656\n",
    "    - выходной слой: (512 + 1) * 10 = 5130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## добавим дополнительный скрытый слой для лучшей производительности\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Выбор функции потерь, оптимизатора, метрик\n",
    "\n",
    "Вызов `model.compile()` указывает, как модель должна обучаться, т. е. какую функцию потерь и оптимизатор использовать и какие показатели оценки качества модели вычислять.\n",
    "\n",
    "* __Функция потерь__ [см. обзор](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "     - Кросс-энтропия (логарифмические потери) для многоклассовой классификации (метка $y_{true}$ должна иметь прямое кодирование (one-hot encoded))\n",
    "     - Используйте бинарную кросс-энтропию для задач бинарной классификации (один выходной нейрон)\n",
    "     - Используйте разреженную категориальную кросс-энтропию, если выход $y_{true}$ закодирован метками 0,1,2,3,...\n",
    "* __Оптимизатор__ [см. обзор](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "     - Любой из доступных оптимизаторов. `RMSprop` и `Adam` обычно работают хорошо.\n",
    "* __Метрики__ [см. обзор](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\n",
    "     - Для мониторинга производительности во время обучения и тестирования, например,  доля верных ответов (accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения всех объектов в методе `compile()` можно указать как текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# коротко\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = 'rmsprop', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большего контроля можно передать названия фактических функций (с параметрами):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import Accuracy\n",
    "\n",
    "# детально\n",
    "model.compile(loss = CategoricalCrossentropy(label_smoothing=0.01),\n",
    "              optimizer = RMSprop(learning_rate=0.001, momentum=0.0),\n",
    "              metrics = [Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Обучение (подгонка)\n",
    "Функция `fit` обучает сеть и возвращает историю потерь при обучении и проверке, а также значения всех метрик за эпоху.\n",
    "\n",
    "```python\n",
    "network.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "```\n",
    "\n",
    "Имеется два важных гиперпараметра:\n",
    "* __Количество эпох__ (epochs): должно быть достаточно, чтобы обеспечить сходимость\n",
    "     * Слишком много: модель начинает переобучаться (или просто теряет время)\n",
    "* __Размер пакета__ (batch_size): часто предпочтительнее небольшие пакеты (например, 32, 64 и т. д.)\n",
    "     * «Зашумленные» обучающие данные  снижают вероятность переобучения\n",
    "         * Большие пакеты хуже обобщаются \n",
    "     * Требуют меньше памяти (особенно в графических процессорах)\n",
    "     * Большие пакеты ускоряют обучение и сходимость достигается за меньшее количество эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Повторяющееся обучение\n",
    "Вызов `model.fit` несколько раз не воссоздает модель с нуля (как это делается в `scikit-learn`), а просто продолжает обучение с сохраненными весами. Для обучения с нуля, например, с разными гиперпараметрами, необходимо воссоздать модель, например, оформив  создание модели как функцию `create_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отслеживание прогресса обучения\n",
    "Вызов `fit` обеспечивает вывод прогресса для каждой эпохи и возвращает объект `history`, содержащий все потери и показатели метрик оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history = model.fit(Xf_train, yf_train, epochs=3, batch_size=64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также указать проверочную (валидационную) выборку, чтобы также возвращались показатели потерь и доли верных ответов на проверочной выборке. Параметр `verbose=0` заглушает вывод данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history = model.fit(Xf_train, yf_train, epochs=3, batch_size=32, verbose=0,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращенная история обучения (объект `history`) содержит данные оценки качества модели (потери и метрики) для каждой эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Прогнозы и оценки\n",
    "Теперь можно вызывать `predict` для генерации прогнозов и оценить качество обученной модели на всем тестовом наборе при помощи `evaluate`.\n",
    "\n",
    "``` python\n",
    "network.predict(X_test)\n",
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Visualize one of the predictions\n",
    "sample_id = 0\n",
    "print('Прогнозируемые вероятности меток:\\n', predictions[sample_id])\n",
    "\n",
    "np.set_printoptions(precision=7)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(4, 4))\n",
    "axes.imshow(X_test[sample_id].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "axes.set_xlabel(\"Истинная метка: {}\".format(y_test[sample_id]))\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Доля верных ответов (accuracy) на тестовой выборке:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Проверка кривых обучения\n",
    "Есть несколько способов проверить кривые обучения\n",
    "* Подождите, пока обучение завершится, затем нарисуйте кривые обучения из возвращенной истории\n",
    "* Добавьте обратный вызов (callback) в функцию `fit`, который перерисовывает кривые  обучения в режиме реального времени при каждом обновлении (пример реализации приведен ниже)\n",
    "* Используйте внешний инструмент, например [TensorBoard](https://www.tensorflow.org/tensorboard/get_started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# For plotting the learning curve in real time\n",
    "class TrainingPlot(keras.callbacks.Callback):\n",
    "    \n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.acc = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "        self.max_acc = 0\n",
    "    \n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.logs.append(logs)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_acc.append(logs.get('val_accuracy'))\n",
    "        self.max_acc = max(self.max_acc, logs.get('val_accuracy'))\n",
    "        \n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.losses) > 1:\n",
    "            \n",
    "            # Clear the previous plot\n",
    "            clear_output(wait=True)\n",
    "            N = np.arange(0, len(self.losses))\n",
    "            \n",
    "            # Plot train loss, train acc, val loss and val acc against epochs passed\n",
    "            plt.figure(figsize=(8,3))\n",
    "            plt.plot(N, self.losses, lw=2, c=\"b\", linestyle=\"-\", label = \"train_loss\")\n",
    "            plt.plot(N, self.acc, lw=2, c=\"r\", linestyle=\"-\", label = \"train_acc\")\n",
    "            plt.plot(N, self.val_losses, lw=2, c=\"b\", linestyle=\":\", label = \"val_loss\")\n",
    "            plt.plot(N, self.val_acc, lw=2, c=\"r\", linestyle=\":\", label = \"val_acc\")\n",
    "            plt.title(\"Потери и доля верных ответов при обучении [эпоха {}, Max Acc {:.4f}]\".format(epoch, self.max_acc))\n",
    "            plt.xlabel(\"Эпоха #\")\n",
    "            plt.ylabel(\"Loss/accuracy\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = TrainingPlot()\n",
    "model = create_model()\n",
    "history = model.fit(Xf_train, yf_train, epochs=25, batch_size=512, verbose=0,\n",
    "                    validation_data=(X_val, y_val), callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ранняя остановка (early stopping)\n",
    "* Нужно прекратить обучение, когда потери на валидационной выборке (или доля верных ответов на валидационной выборке) больше не улучшаются\n",
    "* При этом нужно учитывать, что потери могут быть неровным: используйте скользящее среднее или подождите $k$ шагов без улучшения\n",
    "\n",
    "``` python\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x_train, y_train, epochs=25, batch_size=512, callbacks=[earlystop])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model = create_model()\n",
    "history = model.fit(Xf_train, yf_train, epochs=25, batch_size=512, verbose=0,\n",
    "                    validation_data=(X_val, y_val), callbacks=[plot_losses, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Регуляризация\n",
    "Есть несколько способов регуляризации моделей в случае переобучения:\n",
    "- Получить больше обучающих данных\n",
    "- Уменьшить сеть (например, использовать меньше нейронов в слоях или использовать меньшее количество слоев)\n",
    "- Регуляризировать веса модели (например, с помощью регуляризации L1/L2)\n",
    "- Использовать технику исключения нейронов (dropout)\n",
    "- Пакетная нормализация (batch normalization) также обладает эффектом регуляризации\n",
    "\n",
    "### Регуляризация весов (уменьшение весов)\n",
    "* Регуляризацию весов можно применять к слоям с помощью [регуляризатора](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)\n",
    "    - Регуляризация L1: приводит к _разреженным сетям_ со многими весами, равными нулю\n",
    "    - Регуляризация L2: приводит к большому количеству очень маленьких весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal', \n",
    "                       kernel_regularizer='l1'))\n",
    "model.add(layers.Dense(512, activation='relu', kernel_initializer='he_normal',\n",
    "                       kernel_regularizer='l1'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "model = create_model()\n",
    "history = model.fit(Xf_train, yf_train, epochs=50, batch_size=512, verbose=0,\n",
    "                    validation_data=(X_val, y_val), callbacks=[plot_losses, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Исключение (отсев) нейронов (dropout)\n",
    "* Механизм dropout случайным образом устанавливает некоторое количество функций активации в слое равными нулю. Это позволяет избежать запоминания моделью несущественных связей в данных\n",
    "* Механизм dropout добавляется в модель через слой [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).\n",
    "* Коэффициент отсева (dropout rate) или доля обнулённых выходных значений обычно составляет от 0.1 до 0.5, но этот параметр должен быть настроен на конкретную задачу\n",
    "* Слой dropout может быть добавлен после любого плотного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "history = model.fit(Xf_train, yf_train, epochs=50, batch_size=512, verbose=0,\n",
    "                    validation_data=(X_val, y_val), callbacks=[plot_losses, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пакетная нормализация (batch normalization)\n",
    "* Пакетная нормализация нормализует выходы предыдущего слоя для каждого пакета\n",
    "     * Внутри пакета установите среднюю активацию, близкую к 0, и стандартное отклонение, близкое к 1\n",
    "         * При переходе к обработке другого пакета используется экспоненциальное скользящее среднее и дисперсия пакетов\n",
    "    * Пакетная нормализация позволяет более глубоким сетям быть менее склонными к исчезновению или взрыву градиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(265, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "history = model.fit(Xf_train, yf_train, epochs=50, batch_size=512, verbose=0,\n",
    "                    validation_data=(X_val, y_val), callbacks=[plot_losses, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Комбинирование нескольких регуляризаторов\n",
    "Ведутся споры о том, имеет ли смысл объединять несколько регуляризаторов и в каком порядке. Что работает (или нет) зависит от структуры и размера сети и имеющегося набора данных.\n",
    "\n",
    "Например, поскольку пакетная нормализация уже выполняет некоторую регуляризацию, Dropout может не понадобиться. Однако иногда такая комбинация действительно помогает. Иногда помогает использование Dropout после пакетной нормализации только на самых глубоких уровнях.\n",
    "\n",
    "Пакетная нормализация иногда выполняется перед плотным слоем, но в целом она работает лучше, если применяется после плотного слоя. Аналогично, Dropout можно применить до или после пакетной нормализации. Однако использование Dropout перед пакетной нормализацией приведет к включению нулей в статистику нормализации, что нежелательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "network.add(layers.Dense(265, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(64, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "plot_losses = TrainingPlot()\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "history = network.fit(Xf_train, yf_train, epochs=50, batch_size=512, verbose=0,\n",
    "                      validation_data=(X_val, y_val), callbacks=[plot_losses, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Настройка множественных гиперпараметров\n",
    "* Модуль Keras имеет соответствующую библиотеку настройки [keras-tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) с несколькими методами настройки параметров:\n",
    "- Случайный поиск (RandomSearch)\n",
    "- Гипербанд (Hyperband)\n",
    "- Байесовская оптимизация\n",
    "- Sklearn (для настройки моделей scikit-learn)\n",
    "\n",
    "* Модуль keras-tuner создает папку со всеми результатами для каждого проекта (параметр `project_name`). Нужно будет удалить папку или изменить имя проекта, чтобы запустить его снова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Настроим число нейронов в плотных слоях.\n",
    "    # Выберем оптимальное значение между 32-512.\n",
    "    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "\n",
    "    model.add(keras.layers.Dense(units = hp_units, activation = 'relu', \n",
    "                                 input_shape=(28 * 28,)))\n",
    "    model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(10))\n",
    "\n",
    "    # Настроим шаг обучения для оптимизатора \n",
    "    # Выберем оптимальное значение между 0.01, 0.001 или 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "\n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(build_model, max_trials=5, objective = 'val_accuracy', \n",
    "                        project_name='lab02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполнение кода может занять определенное время\n",
    "tuner.search(Xf_train, yf_train, epochs = 10, validation_data = (X_val, y_val), \n",
    "             callbacks = [TrainingPlot()])\n",
    "\n",
    "# получить оптимальные гиперпараметры\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Вы можете обернуть модели Keras как модели scikit-learn, используя [KerasClassifier](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasClassifier) и использовать любую технику настройки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# pip install scikeras --upgrade\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "def build_model(var_activation='relu',var_optimizer='adam'):\n",
    "    \"\"\" Для построения модели Keras используются аргументы функции. \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(64,activation=var_activation))\n",
    "    model.add(layers.Dense(32,activation=var_activation))\n",
    "    model.add(layers.Dense(16,activation=var_activation))\n",
    "    model.add(layers.Dense(10,activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=var_optimizer,\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# пространство поиска\n",
    "_activations=['tanh','relu','selu']\n",
    "_optimizers=['sgd','adam']\n",
    "_batch_size=[16,32,64]\n",
    "params=dict(var_activation=_activations,\n",
    "            var_optimizer=_optimizers,\n",
    "            batch_size=_batch_size)\n",
    "\n",
    "# обертка\n",
    "model = KerasClassifier(model=build_model,epochs=4,batch_size=16,\n",
    "                        var_optimizer='adam',var_activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# выполнение кода занимает время\n",
    "rscv = RandomizedSearchCV(model, param_distributions=params, cv=2, \n",
    "                          n_iter=3, verbose=1, n_jobs=-1)\n",
    "rscv_results = rscv.fit(Xf_train,yf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Лучший результат равен: {} при использовании параметров {}'.format(\\\n",
    "    rscv_results.best_score_, rscv_results.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функциональный интерфейс Keras\n",
    "\n",
    "В модуле __Keras__ имеется два интерфейса (API) для быстрого построения архитектур нейронных сетей: __последовательный интерфейс__ (Sequential API) и __функциональный интерфейс__ (Functional API). \n",
    "\n",
    "Первый интерфейс позволяет строить только последовательные архитектуры нейронных сетей, в которых выход каждого слоя передается на вход следующего слоя. \n",
    "\n",
    "При помощи функционального интерфейса можно задать нейронную сеть в виде произвольного направленного ациклического графа (DAG или directed acyclic graph), что дает намного больше возможностей для построения сложных моделей. \n",
    "__Направленный ациклический граф__ (DAG) — это ориентированный граф, в котором отсутствуют циклы, но могут быть «параллельные» пути, выходящие из одного узла и разным образом приходящие в конечный узел. \n",
    "В частности, функциональный интерфейс может обрабатывать модели с нелинейной топологией, модели с общими слоями, и модели с несколькими входами или выходами.\n",
    "\n",
    "В качестве примера рассмотрим простую нейронную сеть, созданную при помощи последовательного интерфейса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(shape=(28 * 28,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или через список слоев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.InputLayer(shape=(28 * 28,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воссоздадим эту нейронную сеть при помощи функционального интерфейса. Сначала создаем входные данные нейронной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(28 * 28,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь указывается размерность данных, при этом количество данных всегда опускается. Переменная `inputs` содержит информацию о размерах и типе данных которые будут передаваться в модель: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape, inputs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем новый слой в графе слоев с `inputs` в качестве входных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dense(512, activation='relu')(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим еще один слой в граф слоев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dense(512, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, добавим последний слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layers.Dense(10, activation='softmax', name='OutputLayer')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создаем модель, указав ее входы и выходы в графе слоев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним две модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 по теме №1\n",
    "\n",
    "Загрузите из `keras.datasets` набор данных California Housing price regression dataset (https://keras.io/api/datasets/california_housing/), обучите нейронную сеть прогнозировать медианную цену домов в зависимости от количества комнат в доме, визуализируйте процесс обучения.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "dltfm218",
   "language": "python",
   "name": "dltfm218"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
